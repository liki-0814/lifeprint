import base64
import json
import logging
import time
from typing import Optional

from app.config import settings

logger = logging.getLogger(__name__)


class LLMClient:
    """
    ç»Ÿä¸€çš„å¤§æ¨¡å‹å®¢æˆ·ç«¯ï¼Œæ”¯æŒ OpenAI å’Œ Anthropic ä¸¤ç§ API æ ¼å¼ã€‚

    OpenAI æ ¼å¼å…¼å®¹ï¼šOpenAIã€DashScopeã€DeepSeekã€Groqã€Together ç­‰æ‰€æœ‰ OpenAI å…¼å®¹ APIã€‚
    Anthropic æ ¼å¼å…¼å®¹ï¼šAnthropic Claude ç³»åˆ—ã€‚
    """

    def __init__(
        self,
        provider: Optional[str] = None,
        api_key: Optional[str] = None,
        base_url: Optional[str] = None,
        model: Optional[str] = None,
        vision_model: Optional[str] = None,
    ):
        self.provider = provider or settings.LLM_PROVIDER
        self.api_key = api_key or settings.LLM_API_KEY
        self.base_url = base_url or settings.LLM_BASE_URL
        self.model = model or settings.LLM_MODEL
        self.vision_model = vision_model or settings.LLM_VISION_MODEL

    @classmethod
    def from_user_settings(
        cls,
        llm_provider: Optional[str] = None,
        llm_api_key: Optional[str] = None,
        llm_base_url: Optional[str] = None,
        llm_model: Optional[str] = None,
        llm_vision_model: Optional[str] = None,
    ) -> "LLMClient":
        """ä»ç”¨æˆ·çº§é…ç½®åˆ›å»ºå®¢æˆ·ç«¯ï¼Œä¸ºç©ºçš„å­—æ®µå›é€€åˆ°ç³»ç»Ÿé»˜è®¤é…ç½®ã€‚"""
        return cls(
            provider=llm_provider or None,
            api_key=llm_api_key or None,
            base_url=llm_base_url or None,
            model=llm_model or None,
            vision_model=llm_vision_model or None,
        )

    async def chat(self, prompt: str, system_prompt: str = "") -> str:
        """çº¯æ–‡æœ¬å¯¹è¯ï¼Œè¿”å›æ¨¡å‹å›å¤æ–‡æœ¬ã€‚"""
        if self.provider == "anthropic":
            return await self._anthropic_chat(prompt, system_prompt)
        return await self._openai_chat(prompt, system_prompt)

    async def vision(self, prompt: str, image_paths: list[str], system_prompt: str = "") -> str:
        """å¤šæ¨¡æ€è§†è§‰åˆ†æï¼Œä¼ å…¥å›¾ç‰‡è·¯å¾„åˆ—è¡¨ï¼Œè¿”å›æ¨¡å‹å›å¤æ–‡æœ¬ã€‚"""
        if self.provider == "anthropic":
            return await self._anthropic_vision(prompt, image_paths, system_prompt)
        return await self._openai_vision(prompt, image_paths, system_prompt)

    async def _openai_chat(self, prompt: str, system_prompt: str = "") -> str:
        """OpenAI æ ¼å¼çš„çº¯æ–‡æœ¬å¯¹è¯ã€‚"""
        from openai import AsyncOpenAI

        client = AsyncOpenAI(api_key=self.api_key, base_url=self.base_url)
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})

        logger.info("ğŸ¤– [LLM è¯·æ±‚] provider=openai, model=%s, base_url=%s", self.model, self.base_url)
        logger.info("ğŸ“ [LLM Prompt] (å‰500å­—): %s", prompt[:500])
        start_time = time.time()

        response = await client.chat.completions.create(
            model=self.model,
            messages=messages,
            timeout=120.0,
        )
        result = response.choices[0].message.content or ""
        elapsed = time.time() - start_time

        logger.info("âœ… [LLM å›å¤] è€—æ—¶=%.1fs, å›å¤é•¿åº¦=%då­—ç¬¦", elapsed, len(result))
        logger.info("ğŸ“„ [LLM å›å¤å†…å®¹] (å‰500å­—): %s", result[:500])
        return result

    async def _openai_vision(self, prompt: str, image_paths: list[str], system_prompt: str = "") -> str:
        """OpenAI æ ¼å¼çš„å¤šæ¨¡æ€è§†è§‰åˆ†æã€‚"""
        from openai import AsyncOpenAI

        client = AsyncOpenAI(api_key=self.api_key, base_url=self.base_url)

        content: list[dict] = []
        for path in image_paths:
            with open(path, "rb") as f:
                encoded = base64.b64encode(f.read()).decode("utf-8")
            content.append({
                "type": "image_url",
                "image_url": {"url": f"data:image/jpeg;base64,{encoded}"},
            })
        content.append({"type": "text", "text": prompt})

        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": content})

        logger.info("ğŸ–¼ï¸ [LLM Vision è¯·æ±‚] provider=openai, model=%s, å›¾ç‰‡æ•°=%d", self.vision_model, len(image_paths))
        logger.info("ğŸ“ [Vision Prompt] (å‰500å­—): %s", prompt[:500])
        start_time = time.time()

        response = await client.chat.completions.create(
            model=self.vision_model,
            messages=messages,
            timeout=120.0,
        )
        result = response.choices[0].message.content or ""
        elapsed = time.time() - start_time

        logger.info("âœ… [LLM Vision å›å¤] è€—æ—¶=%.1fs, å›å¤é•¿åº¦=%då­—ç¬¦", elapsed, len(result))
        logger.info("ğŸ“„ [Vision å›å¤å†…å®¹] (å‰500å­—): %s", result[:500])
        return result

    async def _anthropic_chat(self, prompt: str, system_prompt: str = "") -> str:
        """Anthropic æ ¼å¼çš„çº¯æ–‡æœ¬å¯¹è¯ã€‚"""
        import anthropic

        client = anthropic.AsyncAnthropic(api_key=self.api_key, base_url=self.base_url)
        kwargs: dict = {
            "model": self.model,
            "max_tokens": 4096,
            "messages": [{"role": "user", "content": prompt}],
        }
        if system_prompt:
            kwargs["system"] = system_prompt

        logger.info("ğŸ¤– [LLM è¯·æ±‚] provider=anthropic, model=%s, base_url=%s", self.model, self.base_url)
        logger.info("ğŸ“ [LLM Prompt] (å‰500å­—): %s", prompt[:500])
        start_time = time.time()

        response = await client.messages.create(**kwargs)
        result = response.content[0].text if response.content else ""
        elapsed = time.time() - start_time

        logger.info("âœ… [LLM å›å¤] è€—æ—¶=%.1fs, å›å¤é•¿åº¦=%då­—ç¬¦", elapsed, len(result))
        logger.info("ğŸ“„ [LLM å›å¤å†…å®¹] (å‰500å­—): %s", result[:500])
        return result

    async def _anthropic_vision(self, prompt: str, image_paths: list[str], system_prompt: str = "") -> str:
        """Anthropic æ ¼å¼çš„å¤šæ¨¡æ€è§†è§‰åˆ†æã€‚"""
        import anthropic

        client = anthropic.AsyncAnthropic(api_key=self.api_key, base_url=self.base_url)

        content: list[dict] = []
        for path in image_paths:
            with open(path, "rb") as f:
                encoded = base64.b64encode(f.read()).decode("utf-8")
            content.append({
                "type": "image",
                "source": {
                    "type": "base64",
                    "media_type": "image/jpeg",
                    "data": encoded,
                },
            })
        content.append({"type": "text", "text": prompt})

        kwargs: dict = {
            "model": self.vision_model,
            "max_tokens": 4096,
            "messages": [{"role": "user", "content": content}],
        }
        if system_prompt:
            kwargs["system"] = system_prompt

        logger.info("ğŸ–¼ï¸ [LLM Vision è¯·æ±‚] provider=anthropic, model=%s, å›¾ç‰‡æ•°=%d", self.vision_model, len(image_paths))
        logger.info("ğŸ“ [Vision Prompt] (å‰500å­—): %s", prompt[:500])
        start_time = time.time()

        response = await client.messages.create(**kwargs)
        result = response.content[0].text if response.content else ""
        elapsed = time.time() - start_time

        logger.info("âœ… [LLM Vision å›å¤] è€—æ—¶=%.1fs, å›å¤é•¿åº¦=%då­—ç¬¦", elapsed, len(result))
        logger.info("ğŸ“„ [Vision å›å¤å†…å®¹] (å‰500å­—): %s", result[:500])
        return result

    async def test_connection(self) -> dict:
        """æµ‹è¯• API è¿é€šæ€§ï¼Œè¿”å› {"success": bool, "message": str}ã€‚"""
        try:
            result = await self.chat("è¯·å›å¤ï¼šè¿æ¥æˆåŠŸ")
            if result:
                return {"success": True, "message": f"è¿æ¥æˆåŠŸï¼Œæ¨¡å‹å›å¤ï¼š{result[:50]}"}
            return {"success": False, "message": "æ¨¡å‹è¿”å›ç©ºå†…å®¹"}
        except Exception as error:
            return {"success": False, "message": f"è¿æ¥å¤±è´¥ï¼š{str(error)}"}


def get_llm_client(
    llm_provider: Optional[str] = None,
    llm_api_key: Optional[str] = None,
    llm_base_url: Optional[str] = None,
    llm_model: Optional[str] = None,
    llm_vision_model: Optional[str] = None,
) -> LLMClient:
    """è·å– LLM å®¢æˆ·ç«¯å®ä¾‹ï¼Œä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„ç”¨æˆ·çº§é…ç½®ã€‚"""
    return LLMClient.from_user_settings(
        llm_provider=llm_provider,
        llm_api_key=llm_api_key,
        llm_base_url=llm_base_url,
        llm_model=llm_model,
        llm_vision_model=llm_vision_model,
    )
